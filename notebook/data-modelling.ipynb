{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "387d8544-3e66-4c46-a9e4-600eff45113a",
   "metadata": {},
   "source": [
    "#### English to Swahili Translation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8ae0b7-d7bd-4bcf-a641-6a00bed3fb40",
   "metadata": {},
   "source": [
    "#### 1.0 Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cfa57b0-e36f-40dd-82cf-e74c9bac4ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-02 13:47:31.966244: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740bd31d-3289-4d8a-9ae5-8feda985d11b",
   "metadata": {},
   "source": [
    "#### 1.1 loading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "404ab316-5467-4cb9-ad8d-9159f2c1a7a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "469d5874b80d42c28c2280d93120f0fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d26a279403d94241b6abb35c59f401af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e18dffe5bc0847bc8ce3afaaffb40541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the CSV dataset\n",
    "df = load_dataset(\"csv\", data_files=\"../dataset/ensw.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162a3e2a-e933-4eb8-a598-861570e5403d",
   "metadata": {},
   "source": [
    "#### 1.2 Viewing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "739bf077-30b5-4e94-b4a0-0172b2ec6f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset object:\n",
      "\n",
      " DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['English sentence', 'Swahili Translation'],\n",
      "        num_rows: 209155\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset object:\\n\\n\", df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ecd015-546a-4cf9-8b0a-bae7bc7b2bdc",
   "metadata": {},
   "source": [
    "#### 1.3 Viewing the information about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f823493-3abc-47fb-863d-ef177a6de570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset information:\n",
      "\n",
      " {'English sentence': Value(dtype='string', id=None), 'Swahili Translation': Value(dtype='string', id=None)}\n"
     ]
    }
   ],
   "source": [
    "train_dataset = df['train']\n",
    "print(\"Train dataset information:\\n\\n\", train_dataset.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912c2d99-58f6-465c-bea1-3247c9ab0754",
   "metadata": {},
   "source": [
    "#### 1.4  Viewing the split dataset information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d8d8fbb-9836-407b-9af3-b098d80f8a8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Split datasets:\n",
      "\n",
      " DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['English sentence', 'Swahili Translation'],\n",
      "        num_rows: 188239\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['English sentence', 'Swahili Translation'],\n",
      "        num_rows: 20916\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "split_df = df['train'].train_test_split(train_size=0.9, seed=20)\n",
    "print(\"\\nSplit datasets:\\n\\n\", split_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f183e1-a996-42ca-9fc4-c77e44c748d8",
   "metadata": {},
   "source": [
    "\n",
    "#### 1.5 Loading the tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0814b196-d870-4999-8c3a-9f8dff0f0392",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-swc\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"pt\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd3d5a2-12a5-4bd4-a81a-f2cfda89e5f8",
   "metadata": {},
   "source": [
    "#### 1.5 Set the maximum sequence length and define the preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "953fa3fd-4dba-49cd-a813-735df07e0026",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "245c770b-d9b0-4b02-8337-c00cd895b9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = str(examples['English sentence'])\n",
    "    targets = str(examples['Swahili Translation'])\n",
    "    \n",
    "    model_inputs = tokenizer(\n",
    "        inputs, text_target=targets, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    model_inputs['decoder_input_ids'] = model_inputs['input_ids'].clone()\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88d9f25-22f0-460b-b232-d8cce19338f6",
   "metadata": {},
   "source": [
    "#### 1.6 Preprocess the training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "853b7e7c-f88f-457e-b82a-76947d1b6769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff92c015c3b841c5b8bc24d79d7a5141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/188239 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = split_df['train'].map(\n",
    "    preprocess_function, batched=True, num_proc=4, remove_columns=[\"English sentence\", \"Swahili Translation\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f79f988-7093-4661-9e72-671374140b83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03ac670053a940578adb3b9335d4b9b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20916 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_dataset = split_df['test'].map(\n",
    "    preprocess_function, batched=True, remove_columns=[\"English sentence\", \"Swahili Translation\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca73045-d688-4551-ad22-3ec1fb94cbf2",
   "metadata": {},
   "source": [
    "#### 1.7 Define the training arguments and create the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "869a8039-daf7-4945-ad35-405d8b9af28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='../model/',\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    save_steps=1000,\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=20,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    "    logging_dir=\"../logs/\",\n",
    "    logging_steps=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ef99b18-2f3c-4740-82ae-16d31f7fefd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8fb25d-3a0a-433b-9c74-73067a672109",
   "metadata": {},
   "source": [
    "#### 1.8 Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1833977-bcb8-4c20-90c2-b9637f5e5f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='240' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [240/240 54:35, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    trainer.train()\n",
    "except ValueError as e:\n",
    "    print(\"Error during training:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53d45ac-82a2-4787-b06b-06e72f6a2474",
   "metadata": {},
   "source": [
    "#### 1.9 Evaluate the model on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7761c586-e48e-4f8b-873e-fbe62a0ff89f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.145776748657227, 'eval_runtime': 6.9935, 'eval_samples_per_second': 3.003, 'eval_steps_per_second': 0.286, 'epoch': 20.0}\n"
     ]
    }
   ],
   "source": [
    "result = trainer.evaluate()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3ff39a-bf42-453f-b407-6e5aec864399",
   "metadata": {},
   "source": [
    "#### 2.0 Export the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04447f96-a149-4f36-ab33-1b63b8126aa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../model/tokenizer_config.json',\n",
       " '../model/special_tokens_map.json',\n",
       " '../model/vocab.json',\n",
       " '../model/source.spm',\n",
       " '../model/target.spm',\n",
       " '../model/added_tokens.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"../model/\")\n",
    "tokenizer.save_pretrained(\"../model/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c85cbff-4a52-43f5-8e8a-b6875012c0fd",
   "metadata": {},
   "source": [
    "#### 2.1 Creating a pipeline for translation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d8e60f8-9c1a-4751-bdd6-c7e371cbc9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"../model/\",\n",
    "    tokenizer=\"../model/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a81dea-3097-4eeb-b573-d24e4f5c837c",
   "metadata": {},
   "source": [
    "#### 2.2 Prompt the user to enter a sentence for translation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447fa907-9f9a-45e5-ac43-38bdae3001fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter an English sentence for translation to Swahili (type 'exit' to quit):  baby\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated text: Mtoto\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter an English sentence for translation to Swahili (type 'exit' to quit):  Human immunodeficiency virus (HIV) is an infection that attacks the body’s immune system. Acquired immunodeficiency syndrome (AIDS) is the most advanced stage of the disease.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated text: Virusi vya mfumo wa kinga wa binadamu (ADDS) ni virusi vinavyoshambulia mfumo wa kinga wa mwili.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter an English sentence for translation to Swahili (type 'exit' to quit):  ONLY candidates with the above documents will be interviewed. Candidates who present false documents or information will be disqualified.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated text: WATU wanaotaka kujua kusoma hati hizo watahojiwa.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter an English sentence for translation to Swahili (type 'exit' to quit):  Candidates who present false documents or information will be disqualified\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated text: Watendaji wanaoandika hati au habari za uwongo wataharibiwa\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter an English sentence for translation to Swahili (type 'exit' to quit):  Lily, Hopper, and Gus spent their days exploring the enchanting woods, discovering hidden glades, and sharing secrets only they could understand. Hopper would guide them through the thick underbrush with his swift leaps, while Gus would nibble on the sweetest grass, offering bites to his friends\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated text: Kwa kawaida, Hopper, na Gus walitumia siku nyingi wakichunguza misitu yenye kuvutia, kugundua maua yaliyofichwa, na kueleza siri ambazo wangeweza kuelewa tu, na hivyo kuwaongoza kutokana na kelele zake za kuruka kwa kasi, huku Gus akiuma kwenye nyasi tamu zaidi na kuwauma rafiki zake\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    text = input(\"Enter an English sentence for translation to Swahili (type 'exit' to quit): \")\n",
    "    if text == \"exit\":\n",
    "        break\n",
    "    translated_text = translator(text, max_length=max_length, num_beams=5)[0]['generated_text']\n",
    "    print(f\"Translated text: {translated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6249e68-9081-4482-96de-6fa8959bffea",
   "metadata": {},
   "source": [
    "## THANK YOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rogendo/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-12-17 17:14:08.670451: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-17 17:14:08.670720: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-17 17:14:08.772513: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-17 17:14:09.070647: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-17 17:14:12.019812: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# translator pipeline for english to swahili translations\n",
    "eng_swa_model_checkpoint = \"Helsinki-NLP/opus-mt-en-swc\"\n",
    "eng_swa_tokenizer = AutoTokenizer.from_pretrained(\"../model/eng_swa_model/\")\n",
    "eng_swa_model = AutoModelForSeq2SeqLM.from_pretrained(\"../model/eng_swa_model/\")\n",
    "\n",
    "eng_swa_translator = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=eng_swa_model,\n",
    "    tokenizer=eng_swa_tokenizer,\n",
    ")\n",
    "\n",
    "def translate_text_eng_swa(text):\n",
    "    translated_text = eng_swa_translator(text, max_length=128, num_beams=5)[0]['generated_text']\n",
    "    return translated_text\n",
    "\n",
    "# translator pipeline for swahili to english translations\n",
    "swa_eng_model_checkpoint = \"Helsinki-NLP/opus-mt-swc-en\"\n",
    "swa_eng_tokenizer = AutoTokenizer.from_pretrained(\"../model/swa_eng_model/\")\n",
    "swa_eng_model = AutoModelForSeq2SeqLM.from_pretrained(\"../model/swa_eng_model/\")\n",
    "\n",
    "swa_eng_translator = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=swa_eng_model,\n",
    "    tokenizer=swa_eng_tokenizer,\n",
    ")\n",
    "\n",
    "def translate_text_swa_eng(text):\n",
    "    translated_text = swa_eng_translator(text, max_length=128, num_beams=5)[0]['generated_text']\n",
    "    return translated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy_langdetect.spacy_langdetect.LanguageDetector at 0x7f4fffcfffd0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy_langdetect import LanguageDetector\n",
    "\n",
    "\n",
    "# Language detector\n",
    "def get_lang_detector(nlp, name):\n",
    "    return LanguageDetector()\n",
    "\n",
    "# # Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# # Register the language detection factory\n",
    "Language.factory(\"language_detector\", func=get_lang_detector)\n",
    "\n",
    "# # Add the language detection component to the pipeline\n",
    "nlp.add_pipe('language_detector', last=True)\n",
    "\n",
    "# res=\"We went to the club for drinks. We were so drunk\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: en\n",
      "Unaweza kupata safu zake, safu za nguzo, na habari kwa kuandika orodha ya kawaida au kwa kutumia safu za safu\n",
      "correct translation\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def write_to_csv(user_input,translated_text, correct_translation):\n",
    "    # Create a CSV file\n",
    "    with open('../incorrect_translations/translations.csv', 'a', newline='') as file:\n",
    "        # Create a CSV writer\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        # Check if the file is empty, i.e., it does not have a header row yet\n",
    "        if os.path.getsize('translations.csv') == 0:\n",
    "            # Write the column header to the file\n",
    "            writer.writerow(['User Input', 'Translated Text', 'Correct Translation'])\n",
    "\n",
    "\n",
    "        # Write the data to the CSV file\n",
    "        writer.writerow([ user_input, translated_text, correct_translation])\n",
    "\n",
    "\n",
    "\n",
    "# user_text=\"Once upon a time in the lush grasslands of East Africa, a wise old tortoise named Kiume lived a quiet, slow life. One sunny day, he stumbled upon a mischievous hare named Njeru. \"\n",
    "# user_text=\"Njeru, a master of deception, was known for tricking other animals into thinking he was a slower creature. This made him an outcast in the animal kingdom, and he often felt alone.\"\n",
    "# user_text=\"Njeru eventually decided to join forces with Kiume. The two formed an unbreakable bond, with Njeru vowing to change his ways and start treating others with respect.\"\n",
    "# user_text=\"Kiume, sensing Njeru's loneliness, offered his friendship. At first, Njeru hesitated, remembering the tricks he had played on others. But as time went by, he found himself growing fonder of Kiume.\"\n",
    "user_text=\"You can access its rows, columns, and data by using standard indexing or by using column headers\"\n",
    "# language detector here\n",
    "\n",
    "# language detection\n",
    "doc=nlp(user_text)\n",
    "detected_language = doc._.language['language']\n",
    "print(f\"Detected language: {detected_language}\")   \n",
    "\n",
    "if detected_language==\"en\":   \n",
    "    translated_text=translate_text_eng_swa(user_text)\n",
    "    print(translate_text_eng_swa(user_text))\n",
    "elif detected_language=='sw':\n",
    "    translated_text=translate_text_swa_eng(user_text)\n",
    "    print(translate_text_swa_eng(user_text))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "is_translation_correct = input(\"Is translation correct? \")\n",
    "# is_translation_correct = bool(is_translation_correct)\n",
    "if is_translation_correct.lower()=='yes': # If is_translation_correct is True\n",
    "    print(\"correct translation\")\n",
    "    \n",
    "elif  is_translation_correct.lower()=='no': # If is_translation_correct is False\n",
    "    corrected_text = input(\"Enter the corrected text: \")\n",
    "    \n",
    "    print(\"correct translation \" + corrected_text)\n",
    "    write_to_csv(user_text, translated_text, corrected_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # proprocess the saved csv file \n",
    "# def preprocess_csv(file):\n",
    "#     import pandas as pd\n",
    "#     df = pd.read_csv(file)\n",
    "#     # remove rows with missing values (NaN or None)\n",
    "#     df = df.dropna()\n",
    "#     # df = df.\n",
    "#     return df\n",
    "\n",
    "# data = open('../incorrect_translations/translations.csv')\n",
    "# preprocess_csv(data)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User Input</th>\n",
       "      <th>Translated Text</th>\n",
       "      <th>Correct Translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what was the original crime</td>\n",
       "      <td>Uhalifu wa awali ulisababishwa na nini?</td>\n",
       "      <td>uhalifu wa kwanza ulikuwa?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kiume, sensing Njeru's loneliness, offered his...</td>\n",
       "      <td>Kiume, ambaye alitambua upweke wa Njeru, alito...</td>\n",
       "      <td>Kiume ambaye alitambua upweke wa Njeru, alijit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kiume, sensing Njeru's loneliness, offered his...</td>\n",
       "      <td>Kiume, ambaye alitambua upweke wa Njeru, alito...</td>\n",
       "      <td>Kiume, ambaye alitambua upweke wa Njeru, aliji...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Njeru eventually decided to join forces with K...</td>\n",
       "      <td>Mwishowe Njeru aliamua kujiunga na jeshi la Kiume</td>\n",
       "      <td>Mwishowe Njeru aliamua kujiunga na Kiume</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Njeru eventually decided to join forces with K...</td>\n",
       "      <td>Mwishowe Njeru aliamua kujiunga na majeshi pam...</td>\n",
       "      <td>Mwishowe Njeru aliamua kujiunga pamoja na Kium...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          User Input  \\\n",
       "0                       what was the original crime    \n",
       "1  Kiume, sensing Njeru's loneliness, offered his...   \n",
       "2  Kiume, sensing Njeru's loneliness, offered his...   \n",
       "3  Njeru eventually decided to join forces with K...   \n",
       "4  Njeru eventually decided to join forces with K...   \n",
       "\n",
       "                                     Translated Text  \\\n",
       "0            Uhalifu wa awali ulisababishwa na nini?   \n",
       "1  Kiume, ambaye alitambua upweke wa Njeru, alito...   \n",
       "2  Kiume, ambaye alitambua upweke wa Njeru, alito...   \n",
       "3  Mwishowe Njeru aliamua kujiunga na jeshi la Kiume   \n",
       "4  Mwishowe Njeru aliamua kujiunga na majeshi pam...   \n",
       "\n",
       "                                 Correct Translation  \n",
       "0                         uhalifu wa kwanza ulikuwa?  \n",
       "1  Kiume ambaye alitambua upweke wa Njeru, alijit...  \n",
       "2  Kiume, ambaye alitambua upweke wa Njeru, aliji...  \n",
       "3           Mwishowe Njeru aliamua kujiunga na Kiume  \n",
       "4  Mwishowe Njeru aliamua kujiunga pamoja na Kium...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../incorrect_translations/translations.csv\")\n",
    "# remove rows with missing values (NaN or None)\n",
    "df = df.dropna()\n",
    "df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
