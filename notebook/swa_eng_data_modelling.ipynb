{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "387d8544-3e66-4c46-a9e4-600eff45113a"
      },
      "source": [
        "#### English to Swahili Translation Model"
      ],
      "id": "387d8544-3e66-4c46-a9e4-600eff45113a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af8ae0b7-d7bd-4bcf-a641-6a00bed3fb40"
      },
      "source": [
        "#### 1.0 Importing necessary libraries"
      ],
      "id": "af8ae0b7-d7bd-4bcf-a641-6a00bed3fb40"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5DU6qrGPPoU",
        "outputId": "8c578255-832d-4019-fb74-eab15e5ec32c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.17.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.35.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.1.0+cu118)\n",
            "Requirement already satisfied: accelerate>=0.20.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.24.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.17.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install sentencepiece\n",
        "!pip install transformers[torch]\n",
        "!pip install accelerate -U"
      ],
      "id": "s5DU6qrGPPoU"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "u1GK2e08Pcwx"
      },
      "outputs": [],
      "source": [],
      "id": "u1GK2e08Pcwx"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7cfa57b0-e36f-40dd-82cf-e74c9bac4ba4"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from datasets import load_dataset\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "import sentencepiece\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "id": "7cfa57b0-e36f-40dd-82cf-e74c9bac4ba4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "740bd31d-3289-4d8a-9ae5-8feda985d11b"
      },
      "source": [
        "#### 1.1 loading the datasets"
      ],
      "id": "740bd31d-3289-4d8a-9ae5-8feda985d11b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 608,
          "referenced_widgets": [
            "57e6a93897c0474c8016e42955cc5539",
            "b4db58eacc064f3bb40f8d587611a250",
            "c6af8a75b8234a7e8e0eca51f97aaebd"
          ]
        },
        "id": "404ab316-5467-4cb9-ad8d-9159f2c1a7a3",
        "outputId": "77170cf8-05d1-4755-cf34-5c74e1c07434"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "57e6a93897c0474c8016e42955cc5539",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b4db58eacc064f3bb40f8d587611a250",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c6af8a75b8234a7e8e0eca51f97aaebd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load the CSV dataset\n",
        "df = load_dataset(\"csv\", data_files=\"ensw_v2.csv\")"
      ],
      "id": "404ab316-5467-4cb9-ad8d-9159f2c1a7a3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "162a3e2a-e933-4eb8-a598-861570e5403d"
      },
      "source": [
        "#### 1.2 Viewing the dataset"
      ],
      "id": "162a3e2a-e933-4eb8-a598-861570e5403d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "739bf077-30b5-4e94-b4a0-0172b2ec6f3e",
        "outputId": "39132342-8979-4b20-f6b3-07c4c4217d2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset object:\n",
            "\n",
            " DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['English sentence', 'Swahili Translation', 'Unnamed: 2'],\n",
            "        num_rows: 193521\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "print(\"Dataset object:\\n\\n\", df)"
      ],
      "id": "739bf077-30b5-4e94-b4a0-0172b2ec6f3e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24ecd015-546a-4cf9-8b0a-bae7bc7b2bdc"
      },
      "source": [
        "#### 1.3 Viewing the information about the dataset"
      ],
      "id": "24ecd015-546a-4cf9-8b0a-bae7bc7b2bdc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5f823493-3abc-47fb-863d-ef177a6de570",
        "outputId": "19aa1547-2fb6-4fa6-e438-6ce4e0351cfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train dataset information:\n",
            "\n",
            " {'English sentence': Value(dtype='string', id=None), 'Swahili Translation': Value(dtype='string', id=None), 'Unnamed: 2': Value(dtype='float64', id=None)}\n"
          ]
        }
      ],
      "source": [
        "train_dataset = df['train']\n",
        "print(\"Train dataset information:\\n\\n\", train_dataset.features)"
      ],
      "id": "5f823493-3abc-47fb-863d-ef177a6de570"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "912c2d99-58f6-465c-bea1-3247c9ab0754"
      },
      "source": [
        "#### 1.4  Viewing the split dataset information\n"
      ],
      "id": "912c2d99-58f6-465c-bea1-3247c9ab0754"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9d8d8fbb-9836-407b-9af3-b098d80f8a8b",
        "outputId": "df8a055d-76ef-47a6-b2e7-101487bc929b",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Split datasets:\n",
            "\n",
            " DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['English sentence', 'Swahili Translation', 'Unnamed: 2'],\n",
            "        num_rows: 174168\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['English sentence', 'Swahili Translation', 'Unnamed: 2'],\n",
            "        num_rows: 19353\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "split_df = df['train'].train_test_split(train_size=0.9, seed=20)\n",
        "print(\"\\nSplit datasets:\\n\\n\", split_df)"
      ],
      "id": "9d8d8fbb-9836-407b-9af3-b098d80f8a8b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25f183e1-a996-42ca-9fc4-c77e44c748d8"
      },
      "source": [
        "\n",
        "#### 1.5 Loading the tokenizer and model"
      ],
      "id": "25f183e1-a996-42ca-9fc4-c77e44c748d8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0814b196-d870-4999-8c3a-9f8dff0f0392",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "model_checkpoint = \"Helsinki-NLP/opus-mt-en-swc\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"pt\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
      ],
      "id": "0814b196-d870-4999-8c3a-9f8dff0f0392"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dd3d5a2-12a5-4bd4-a81a-f2cfda89e5f8"
      },
      "source": [
        "# 1.5 Set the maximum sequence length and define the preprocessing function"
      ],
      "id": "6dd3d5a2-12a5-4bd4-a81a-f2cfda89e5f8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "953fa3fd-4dba-49cd-a813-735df07e0026"
      },
      "outputs": [],
      "source": [
        "max_length = 128"
      ],
      "id": "953fa3fd-4dba-49cd-a813-735df07e0026"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "245c770b-d9b0-4b02-8337-c00cd895b9da"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    inputs = str(examples['English sentence'])\n",
        "    targets = str(examples['Swahili Translation'])\n",
        "\n",
        "    model_inputs = tokenizer(\n",
        "        inputs, text_target=targets, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
        "    )\n",
        "    model_inputs['decoder_input_ids'] = model_inputs['input_ids'].clone()\n",
        "\n",
        "    return model_inputs"
      ],
      "id": "245c770b-d9b0-4b02-8337-c00cd895b9da"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f88d9f25-22f0-460b-b232-d8cce19338f6"
      },
      "source": [
        "#### 1.6 Preprocess the training and validation sets"
      ],
      "id": "f88d9f25-22f0-460b-b232-d8cce19338f6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8-tCVnGQLYw"
      },
      "outputs": [],
      "source": [],
      "id": "Y8-tCVnGQLYw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "853b7e7c-f88f-457e-b82a-76947d1b6769"
      },
      "outputs": [],
      "source": [
        "# # loading the tokenizer and model\n",
        "# model_checkpoint = \"Helsinki-NLP/opus-mt-en-swc\"\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"pt\")\n",
        "# model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
        "# max_length = 128\n",
        "# def preprocess_function(examples):\n",
        "#     inputs = str(examples['English sentence'])\n",
        "#     targets = str(examples['Swahili Translation'])\n",
        "\n",
        "#     model_inputs = tokenizer(\n",
        "#         inputs, text_target=targets, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
        "#     )\n",
        "#     model_inputs['decoder_input_ids'] = model_inputs['input_ids'].clone()\n",
        "\n",
        "#     return model_inputs\n",
        "train_dataset = split_df['train'].map(\n",
        "    preprocess_function, batched=True, num_proc=4, remove_columns=[\"English sentence\", \"Swahili Translation\"]\n",
        ")\n"
      ],
      "id": "853b7e7c-f88f-457e-b82a-76947d1b6769"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5f79f988-7093-4661-9e72-671374140b83"
      },
      "outputs": [],
      "source": [
        "eval_dataset = split_df['test'].map(\n",
        "    preprocess_function, batched=True, remove_columns=[\"English sentence\", \"Swahili Translation\"]\n",
        ")"
      ],
      "id": "5f79f988-7093-4661-9e72-671374140b83"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ca73045-d688-4551-ad22-3ec1fb94cbf2"
      },
      "source": [
        "#### 1.7 Define the training arguments and create the trainer"
      ],
      "id": "2ca73045-d688-4551-ad22-3ec1fb94cbf2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "869a8039-daf7-4945-ad35-405d8b9af28c"
      },
      "outputs": [],
      "source": [
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir='../model/',\n",
        "    predict_with_generate=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=1000,\n",
        "    save_steps=1000,\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=50,\n",
        "    weight_decay=0.01,\n",
        "    push_to_hub=False,\n",
        "    logging_dir=\"../logs/\",\n",
        "    logging_steps=500,\n",
        ")"
      ],
      "id": "869a8039-daf7-4945-ad35-405d8b9af28c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ef99b18-2f3c-4740-82ae-16d31f7fefd6"
      },
      "outputs": [],
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        ")"
      ],
      "id": "7ef99b18-2f3c-4740-82ae-16d31f7fefd6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e8fb25d-3a0a-433b-9c74-73067a672109"
      },
      "source": [
        "#### 1.8 Train the model"
      ],
      "id": "9e8fb25d-3a0a-433b-9c74-73067a672109"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "f1833977-bcb8-4c20-90c2-b9637f5e5f9e",
        "outputId": "ebf467d8-3088-4e3c-d0b3-d7f140f7190a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='360' max='360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [360/360 02:05, Epoch 30/30]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "try:\n",
        "    trainer.train()\n",
        "except ValueError as e:\n",
        "    print(\"Error during training:\", e)"
      ],
      "id": "f1833977-bcb8-4c20-90c2-b9637f5e5f9e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a53d45ac-82a2-4787-b06b-06e72f6a2474"
      },
      "source": [
        "#### 1.9 Evaluate the model on the validation set"
      ],
      "id": "a53d45ac-82a2-4787-b06b-06e72f6a2474"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7761c586-e48e-4f8b-873e-fbe62a0ff89f",
        "outputId": "8f60b3db-a8c9-44fe-ec63-5e4915436bc3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2/2 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 6.103726387023926, 'eval_runtime': 0.169, 'eval_samples_per_second': 124.252, 'eval_steps_per_second': 11.834, 'epoch': 30.0}\n"
          ]
        }
      ],
      "source": [
        "result = trainer.evaluate()\n",
        "print(result)"
      ],
      "id": "7761c586-e48e-4f8b-873e-fbe62a0ff89f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df3ff39a-bf42-453f-b407-6e5aec864399"
      },
      "source": [
        "#### 2.0 Export the trained model"
      ],
      "id": "df3ff39a-bf42-453f-b407-6e5aec864399"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qAt-IVzzDH_K"
      },
      "outputs": [],
      "source": [
        "# saving the model as a pkl  file\n",
        "import pickle\n",
        "pickle_out = open(\"model.pkl\", mode = \"wb\")\n",
        "pickle.dump(model, pickle_out)\n",
        "pickle_out.close()\n",
        "\n"
      ],
      "id": "qAt-IVzzDH_K"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "04447f96-a149-4f36-ab33-1b63b8126aa8",
        "outputId": "bfa204a5-3672-415c-fb1c-11bc9398559b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('../model/tokenizer_config.json',\n",
              " '../model/special_tokens_map.json',\n",
              " '../model/vocab.json',\n",
              " '../model/source.spm',\n",
              " '../model/target.spm',\n",
              " '../model/added_tokens.json')"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.save_pretrained(\"../model/\")\n",
        "tokenizer.save_pretrained(\"../model/\")"
      ],
      "id": "04447f96-a149-4f36-ab33-1b63b8126aa8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c85cbff-4a52-43f5-8e8a-b6875012c0fd"
      },
      "source": [
        "#### 2.1 Creating a pipeline for translation\n"
      ],
      "id": "7c85cbff-4a52-43f5-8e8a-b6875012c0fd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9d8e60f8-9c1a-4751-bdd6-c7e371cbc9ce"
      },
      "outputs": [],
      "source": [
        "translator = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=\"../model/\",\n",
        "    tokenizer=\"../model/\",\n",
        ")"
      ],
      "id": "9d8e60f8-9c1a-4751-bdd6-c7e371cbc9ce"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3a81dea-3097-4eeb-b573-d24e4f5c837c"
      },
      "source": [
        "#### 2.2 Prompt the user to enter a sentence for translation\n"
      ],
      "id": "f3a81dea-3097-4eeb-b573-d24e4f5c837c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "447fa907-9f9a-45e5-ac43-38bdae3001fe"
      },
      "outputs": [],
      "source": [
        "while True:\n",
        "    text = input(\"Andika sentensi Unayotaka itafsiriwe kwa Kiingereza (andika 'exit' ndio utoke): \")\n",
        "    if text == \"exit\":\n",
        "        break\n",
        "    translated_text = translator(text, max_length=max_length, num_beams=5)[0]['generated_text']\n",
        "    print(f\"Translated text: {translated_text}\")"
      ],
      "id": "447fa907-9f9a-45e5-ac43-38bdae3001fe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6249e68-9081-4482-96de-6fa8959bffea"
      },
      "source": [
        "# To enforce The biderectional model,\n",
        "we have to have two models, One that translates from English to Swahili and another which translates from Swahili to English\n",
        "# 3.0 Load the swaeng_v2 Dataset"
      ],
      "id": "f6249e68-9081-4482-96de-6fa8959bffea"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_qRE2nwPQH5P"
      },
      "outputs": [],
      "source": [
        "# Load the Swa-Eng CSV dataset\n",
        "df = load_dataset(\"csv\", data_files=\"sweng_v2.csv\")"
      ],
      "id": "_qRE2nwPQH5P"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wy_usyFros_F"
      },
      "source": [
        "# 3.1 Viewing the dataset\n",
        "\n"
      ],
      "id": "Wy_usyFros_F"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKsOuPriorzB",
        "outputId": "7a606804-3510-4d1e-f9f2-5853ab71c228"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset object:\n",
            "\n",
            " DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['Swahili sentence', 'English Translation'],\n",
            "        num_rows: 209498\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"Dataset object:\\n\\n\", df)"
      ],
      "id": "MKsOuPriorzB"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYM6OtVFpU6L",
        "outputId": "b95fa488-5eb0-4929-8dde-f8be1b3bd929"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset information:\n",
            "\n",
            " {'Swahili sentence': Value(dtype='string', id=None), 'English Translation': Value(dtype='string', id=None)}\n"
          ]
        }
      ],
      "source": [
        "# Viewing more info about the dataset\n",
        "\n",
        "train_dataset = df['train']\n",
        "print(\"Train dataset information:\\n\\n\", train_dataset.features)"
      ],
      "id": "bYM6OtVFpU6L"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFrFeF0ipeIZ"
      },
      "source": [
        "# 3.2 Viewing and Spliting the dataset"
      ],
      "id": "iFrFeF0ipeIZ"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvADU14gphQT",
        "outputId": "daef2208-e88a-4513-a8c4-5fa64efce3ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Split datasets:\n",
            "\n",
            " DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['Swahili sentence', 'English Translation'],\n",
            "        num_rows: 188548\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['Swahili sentence', 'English Translation'],\n",
            "        num_rows: 20950\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "split_df = df['train'].train_test_split(train_size=0.9, seed=20)\n",
        "print(\"\\nSplit datasets:\\n\\n\", split_df)"
      ],
      "id": "WvADU14gphQT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SzDsmEapwzW"
      },
      "source": [
        "# 3.3 Loading Tokenizer and Model"
      ],
      "id": "9SzDsmEapwzW"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "EbBTvUrip0Up"
      },
      "outputs": [],
      "source": [
        "model_checkpoint = \"Helsinki-NLP/opus-mt-swc-en\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"pt\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
      ],
      "id": "EbBTvUrip0Up"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiEPE0ZfqDet"
      },
      "source": [
        "# 3.4 Set the maximum sequence length and define the preprocessing function"
      ],
      "id": "WiEPE0ZfqDet"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "SxfPZ1E8qN6x"
      },
      "outputs": [],
      "source": [
        "max_length = 128"
      ],
      "id": "SxfPZ1E8qN6x"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "D1IRT7lRva3D"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    inputs = str(examples['English Translation'])\n",
        "    targets = str(examples['Swahili sentence'])\n",
        "\n",
        "    model_inputs = tokenizer(\n",
        "        inputs, text_target=targets, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
        "    )\n",
        "    model_inputs['decoder_input_ids'] = model_inputs['input_ids'].clone()\n",
        "\n",
        "    return model_inputs"
      ],
      "id": "D1IRT7lRva3D"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGJFfkIivjLD"
      },
      "source": [
        "# 3.5 Preprocess the training and validation sets"
      ],
      "id": "IGJFfkIivjLD"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "KtmIZIYavn4K"
      },
      "outputs": [],
      "source": [
        "train_dataset = split_df['train'].map(\n",
        "    preprocess_function, batched=True, num_proc=4, remove_columns=[\"Swahili sentence\", \"English Translation\"]\n",
        ")\n"
      ],
      "id": "KtmIZIYavn4K"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Xly2gXJkvvuf"
      },
      "outputs": [],
      "source": [
        "eval_dataset = split_df['test'].map(\n",
        "    preprocess_function, batched=True, remove_columns=[\"Swahili sentence\", \"English Translation\"]\n",
        ")"
      ],
      "id": "Xly2gXJkvvuf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdLPwCHsv1In"
      },
      "source": [
        "# 3.6 Define the training arguments and create the trainer"
      ],
      "id": "qdLPwCHsv1In"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r--9bMEPCQhD"
      },
      "id": "r--9bMEPCQhD",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "jA-nL-p7v0FP"
      },
      "outputs": [],
      "source": [
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir='../model/',\n",
        "    predict_with_generate=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=1000,\n",
        "    save_steps=1000,\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=20,\n",
        "    weight_decay=0.01,\n",
        "    push_to_hub=False,\n",
        "    logging_dir=\"../logs/\",\n",
        "    logging_steps=500,\n",
        ")"
      ],
      "id": "jA-nL-p7v0FP"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ZNjkuRiWwAOI"
      },
      "outputs": [],
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        ")"
      ],
      "id": "ZNjkuRiWwAOI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGQxyuiEvz0l"
      },
      "source": [
        "# 3.7 Training the model"
      ],
      "id": "MGQxyuiEvz0l"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "hd7c8IFHwGKd",
        "outputId": "62d67d10-0833-4540-fd66-2602a8afcd4b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='240' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [240/240 01:30, Epoch 20/20]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "try:\n",
        "    trainer.train()\n",
        "except ValueError as e:\n",
        "    print(\"Error during training:\", e)"
      ],
      "id": "hd7c8IFHwGKd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7QcH_J_wVIB"
      },
      "source": [
        "# 3.8 Model Evaluation"
      ],
      "id": "x7QcH_J_wVIB"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "DDJHFdJZx7hr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "3d38bedc-0ef7-4981-9344-0fcaa62360d0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2/2 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 5.32317590713501, 'eval_runtime': 0.2101, 'eval_samples_per_second': 99.935, 'eval_steps_per_second': 9.518, 'epoch': 20.0}\n"
          ]
        }
      ],
      "source": [
        "result = trainer.evaluate()\n",
        "print(result)"
      ],
      "id": "DDJHFdJZx7hr"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "XbLj5oAKx4MG"
      },
      "outputs": [],
      "source": [
        "\n",
        "# saving the model as a pkl  file\n",
        "import pickle\n",
        "pickle_out = open(\"swa_eng_model.pkl\", mode = \"wb\")\n",
        "pickle.dump(model, pickle_out)\n",
        "pickle_out.close()\n",
        "\n"
      ],
      "id": "XbLj5oAKx4MG"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "wrwD2hDZx-Vu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "902009ea-19e9-4705-ee48-de6359ffb766"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./tokenizer_config.json',\n",
              " './special_tokens_map.json',\n",
              " './vocab.json',\n",
              " './source.spm',\n",
              " './target.spm',\n",
              " './added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "model.save_pretrained(\"./\")\n",
        "tokenizer.save_pretrained(\"./\")"
      ],
      "id": "wrwD2hDZx-Vu"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUJ422T7yEya"
      },
      "source": [
        "# 4.1 Creating pipeline for Translation"
      ],
      "id": "sUJ422T7yEya"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "JLFjYcqjyLcO"
      },
      "outputs": [],
      "source": [
        "translator = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=\"./\",\n",
        "    tokenizer=\"./\",\n",
        ")"
      ],
      "id": "JLFjYcqjyLcO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRZ24qxFIm5-"
      },
      "source": [
        "# 4.2 Prompt the user to enter a sentence for translation\n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "vRZ24qxFIm5-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUbuAP5mIr0_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bac21b10-524f-446e-88cf-1e5807b6b458"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Andika Sentensi yako itafsiriwe (andika 'exit' to quit): bwana mkubwa\n",
            "Translated text: The master\n",
            "Andika Sentensi yako itafsiriwe (andika 'exit' to quit): habari yako mzee?\n",
            "Translated text: about your old age?\n",
            "Andika Sentensi yako itafsiriwe (andika 'exit' to quit): chakula ki tayari\n",
            "Translated text: food ready\n",
            "Andika Sentensi yako itafsiriwe (andika 'exit' to quit): niliufanya mtihani \n",
            "Translated text: I made a test\n",
            "Andika Sentensi yako itafsiriwe (andika 'exit' to quit): chajio\n",
            "Translated text: mjiji\n",
            "Andika Sentensi yako itafsiriwe (andika 'exit' to quit): mjini\n",
            "Translated text: town\n",
            "Andika Sentensi yako itafsiriwe (andika 'exit' to quit): mji\n",
            "Translated text: town\n",
            "Andika Sentensi yako itafsiriwe (andika 'exit' to quit): kijiji\n",
            "Translated text: village\n",
            "Andika Sentensi yako itafsiriwe (andika 'exit' to quit): hey boy\n",
            "Translated text: hey boy\n",
            "Andika Sentensi yako itafsiriwe (andika 'exit' to quit): wewe kijana\n",
            "Translated text: You young\n",
            "Andika Sentensi yako itafsiriwe (andika 'exit' to quit): ungali kijana\n",
            "Translated text: You're still young\n",
            "Andika Sentensi yako itafsiriwe (andika 'exit' to quit): wewe ni kijana mdogo\n",
            "Translated text: You're a young boy\n",
            "Andika Sentensi yako itafsiriwe (andika 'exit' to quit): wewe ni msichana mdogo\n",
            "Translated text: You're a little girl\n"
          ]
        }
      ],
      "source": [
        "while True:\n",
        "    text = input(\"Andika Sentensi yako itafsiriwe (andika 'exit' to quit): \")\n",
        "    if text == \"exit\":\n",
        "        break\n",
        "\n",
        "    translated_text = translator(text, max_length=max_length, num_beams=5)[0]['generated_text']\n",
        "    print(f\"Translated text: {translated_text}\")"
      ],
      "id": "tUbuAP5mIr0_"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dmUryWoYEHHa"
      },
      "id": "dmUryWoYEHHa",
      "execution_count": 22,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
